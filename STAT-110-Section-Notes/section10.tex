\documentclass[11.5pt]{article}
\usepackage{stat110}

\title{Inequalities, LLN, CLT, Markov Chains}
\sectionnum{10}

\author{\shira, \tim, \creds}

\SOLUTION

\begin{document}

\maketitle

\begin{notes}

\section*{Statistical Inequalities}

\begin{table}[htb!]
   \centering
   \setlength{\extrarowheight}{3pt}
	\begin{tabular}{cccc}
	%\toprule
	\textbf{Cauchy-Schwarz Inequality} & ~ & ~ & \textbf{Jensen's Inequality} \\
	\midrule
	$|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}$ & ~ & ~ & for $g$ convex, $\hspace{.5 cm} E(g(X)) \geq g(E(X))$ \\
	$E(XY)^2 \leq E\left(X^2\right)E\left(Y^2\right)$ & ~ & ~ & for $g$ concave, $\hspace{.5 cm} E(g(X)) \leq g(E(X))$ \\
	~ & ~ & ~ & ~ \\
	%\midrule
	\textbf{Markov's Inequality} & ~ & ~ & \textbf{Chevychev's Inequality} \\
	\midrule
	$P(|X| \geq a) \leq \frac{E|X|}{a}$ & ~ & ~ & $P(|X - E(X)| \geq a) \leq \frac{Var(X)}{a^2}$ \\
	~ & ~ & ~ & $P(|X - E(X)| \geq a\sigma_X) \leq \frac{1}{a^2}$ \\
	%\bottomrule
	\end{tabular}
\end{table}

\section*{$\chi^2$ Distribution (Continuous)}
Let us say that $X$ is distributed $\chi^2_n$. We know the following:
\begin{description}
	\item[Story] A Chi-Squared(n) is a sum of $n$ independent squared normals.
	\item[Example]	The sum of squared errors are distributed $\chi^2_n$
	\item[PDF] The PDF of a $\chi^2_1$ is:
\begin{eqnarray*}
f(w) = \frac{1}{\sqrt{2\pi w}}e^{-w/2}
\hspace{1 in}
w \in [0, \infty)
\end{eqnarray*}
	\item[Properties and Representations]
\end{description}
\vspace{-.4 cm}
	\begin{eqnarray*}
		E(\chi^2_n) = n && Var(X) = 2n \\
		\chi_n^2 &\sim& \Gam\left(\frac{n}{2}, \frac{1}{2}\right) \\
		\chi_n^2 &=& Z_1^2 + Z_2^2 + \dots + Z_n^2 \hspace{1 cm} Z \sim^{i.i.d.} \N(0, 1)
	\end{eqnarray*}

\section*{Law of Large Numbers (LLN)}
Let us have $X_1, X_2, X_3 \dots$ be i.i.d.. We define $\bar{X}_n = \frac{X_1 + X_2 + X_3 + \dots + X_n}{n}$ The Law of Large Numbers states that as $n \longrightarrow \infty$, $\bar{X}_n \longrightarrow E(X)$.

\section*{Central Limit Theorem (CLT)}
\subsection*{Approximation using CLT}
We use $\dot{\,\sim\,}$ to denote \emph{is approximately distributed}. We can use the central limit theorem when we have a random variable, $Y$ that is a sum of $n$ i.i.d. random variables with $n$ large. Let us say that $E(Y) = \mu_Y$ and $\var(Y) = \sigma^2_Y$. We have that:
\[Y \dot{\,\sim\,} \N(\mu_Y, \sigma^2_Y)\]

When we use central limit theorem to estimate $Y$, we usually have $Y = X_1 + X_2 + \dots + X_n$ or $Y = \bar{X}_n= \frac{1}{n}(X_1 + X_2 + \dots + X_n)$. Specifically, if we say that each of the $X_i$ have mean $\mu_X$ and $\sigma^2_X$, then we have the following approximations.

\[ X_1 + X_2 + \dots + X_n \dot{\,\sim\,} \N(n\mu_X, n\sigma^2_X) \]
\[ \bar{X}_n = \frac{1}{n}(X_1 + X_2 + \dots + X_n) \dot{\,\sim\,} \N(\mu_X, \frac{\sigma^2_X}{n}) \]


\subsection*{Asymptotic Distributions using CLT}

We use $\xrightarrow{d}$ to denote \emph{converges in distribution to} as $n \longrightarrow \infty$. These are the same results as the previous section, only letting $n \longrightarrow \infty$ and not letting our normal distribution have any $n$ terms.
\[\frac{1}{\sigma\sqrt{n}} (X_1 + \dots + X_n - n\mu_X) \xrightarrow{d} \N(0, 1)\]
\[\frac{\bar{X}_n - \mu_X}{\sfrac{\sigma}{\sqrt{n}}} \xrightarrow{d} \N(0, 1)\]


\section*{Markov Chains}
A Markov Chain is a walk along a discrete \textbf{state space} \{$1, 2, \dots, M$\}. We let $X_t$ denote which element of the state space the walk is on at time $t$. The Markov Chain is the set of random variables denoting where the walk is at all points in time, $\{X_0, X_1, X_2, \dots \}$. Each $X_i$ takes on values that are in the state space, so if $X_1 = 3$, then at time 1, we are at state 3. 

What makes such a sequence of random variables a Markov Chain is the \textbf{Markov Property}, which says that if you want to predict where the chain is at at a future time, you only need to use the present state, and not any past information. In other words, the \emph{given the present, the future and past are conditionally independent}. 

Mathematically, this says: 
\[P(X_{n+1} = j | X_0 = i_0, X_1 = i_1, \dots, X_n = i_n) = P(X_{n+1} = j | X_n = i_n)\]
In words: Given that my history of states has been $i_0, i_2 \ldots i_n$, the distribution of where my next state will be doesn't depend on any of that history besides $i_n$, the most recent state. 
\section*{State Properties}
A state is either recurrent or transient.
\begin{itemize}
\item If you start at a \textbf{Recurrent State}, then you will always return back to that state at some point in the future.  \emph{You can leave, but you'll always return at some point.} 
\item Otherwise you are at a \textbf{Transient State}. There is some probability that once you leave you will never return. \emph{There's a chance that you'll leave and never come back}
\end{itemize}
A state is either periodic or aperiodic.
\begin{itemize}
\item If you start at a \textbf{Periodic State} of period $k$, then the GCD of all of the possible number steps it would take to return back is  $k$ (which should be $> 1)$.
\item Otherwise you are at an \textbf{Aperiodic State.} The GCD of all of the possible number of steps it would take to return back is 1.
\end{itemize}


\section*{Transition Matrix}
Element $q_{ij}$ in square transition matrix Q is the probability that the chain goes from state $i$ to state $j$, or more formally:
\[q_{ij} = P(X_{n+1} = j | X_n = i)\]

To find the probability that the chain goes from state $i$ to state $j$ in $m$ steps, take the $(i, j)^\textnormal{th}$ element of $Q^m$.
\[q^{(m)}_{ij} = P(X_{n+m} = j | X_n = i)\]
If $X_0$ is distributed according to row-vector PMF $\vec{p}$ (e.g. $p_j = P(X_0 = i_j)$), then the marginal PMF of $X_n$ is $\vec{p}Q^n$.



\section*{Chain Properties}
A chain is \textbf{irreducible} if you can get from anywhere to anywhere. An irreducible chain must have all of its states recurrent. A chain is \textbf{periodic} if any of its states are periodic, and is \textbf{aperiodic} if none of its states are periodic. In an irreducible chain, all states have the same period. \\

A chain is \textbf{reversible} with respect to $\vec{s}$ if $s_iq_{ij} = s_jq_{ji}$ for all $i, j$.  A reversible chain running on $\vec{s}$ is indistinguishable whether it is running forwards in time or backwards in time. Examples of reversible chains include random walks on undirected networks, or any chain with $q_{ij} = q_{ji}$, where the Markov chain would be stationary with respect to $\vec{s} = (\frac{1}{M}, \frac{1}{M}, \dots, \frac{1}{M})$. \\

\textbf{Reversibility Condition Implies Stationarity} - If you have a PMF $\vec{s}$ on a Markov chain with transition matrix $Q$, then $s_iq_{ij} = s_jq_{ji}$ for all $i, j$ implies that $s$ is stationary.


\section*{Stationary Distribution}

Let us say that the vector $\vec{p} = (p_1, p_2, \dots, p_M)$ is a possible and valid PMF of where the Markov Chain is at at a certain time. We will call this vector the stationary distribution, $\vec{s}$, if it satisfies $\vec{s}Q = \vec{s}$. As a consequence, if $X_t$ has the stationary distribution, then all future $X_{t+1}, X_{t + 2}, \dots$ also has the stationary distribution. 
\begin{itemize}
\item If a Markov Chain is irreducible, then it has a unique stationary distribution. In addition, all entries of this stationary distribution are non-zero (which could have been inferred from the fact that all states are recurrent).
\begin{itemize}
\item \textbf{Counterexample: } In the Gambler's Ruin problem, which is not irreducible, what ultimately happens to the chain can either be that one's money is always $0$ or always $N$. 
\end{itemize} 
\item If a Markov Chain is irreducible \textbf{and} aperiodic, then it has a unique stationary distribution $\vec{s}$ and $$ \lim_{n \to \infty} P(X_n = i) = \vec{s}_i$$
 meaning that the chain \textit{converges} to the stationary distribution. 
\begin{itemize}
\item \textbf{Counterexample: } Imagine a Markov chain which is just a cycle, and hence is periodic. Then, depending on where we start, $P(X_n = i)$ will be either 0 or 1 deterministically, and surely won't converge to the stationary distribution, which is uniform across all nodes in the cycle. 
\end{itemize}
\end{itemize}

For irreducible, aperiodic chains, the stationary distribution exists, is unique, and $s_i$ is the long-run probability of a chain being at state $i$. The expected number of steps to return back to $i$ starting from $i$ is $1/s_i$ To solve for the stationary distribution, you can solve for $\vec{s}Q = \vec{s}$ or $(Q^T - I)\vec{s}^T = 0$. The stationary distribution is uniform if the columns of $Q$ sum to 1.



\section*{Random Walk on Undirected Network}
If you have a certain number of nodes with undirected edges between them, and a chain can pick any edge uniformly at random and move to another node, then this is a random walk on an undirected network. The stationary distribution can be easily calculated. Let $d_i$ be the degree of the $i$th node, meaning the number of edges connected to this node. Then, we have: 
$$ \vec{s}_i = \frac{d_i}{\sum_i d_i}$$

For example, in the below graph:
\begin{center}
\includegraphics[scale=0.5]{nodes.jpg}
\end{center}

The stationary distribution would be proportional to: 
$(w_1, w_2, w_3, w_4, w_5, w_6) = (2, 3, 2, 3, 3, 1)$ and therefore, it would be $\left(\frac{2}{14},\frac{3}{14},\frac{2}{14},\frac{3}{14},\frac{3}{14},\frac{1}{14} \right)$

\end{notes}
\newpage
\section*{Practice Problems} 

\begin{comment}
\begin{exercise}{Two Coins}
One of two identical-looking coins is picked from a hat randomly, where one coin has probability $p_1$ of Heads and the other has probability $p_2$ of Heads. Let $X$ be the number of Heads after flipping the chosen coin $n$ times. Find the mean and variance of $X$
\end{exercise}

\begin{solution}{2.5}
We use the law of total expectation to condition on which coin we selected. Let $I$ be an indicator random variable of selecting coin 1. Then we have: 

$$ E(X) = E(X | I = 1) \cdot P( I = 1) + E(X | I = 0) \cdot P(I = 0) $$
which simplifies to: 
$$ p_1 \cdot \frac{1}{2} + p_2 \cdot \frac{1}{2} = \frac{p_1 + p_2}{2}$$ 

We will use Eve's Law to calculate the variance: 
\begin{align*}
\var(X) &= E( \var{(X| I)}) + \var{(E(X| I))} 
\end{align*}
Now, we write $\var(X | I)$ and $E(X | I)$ in terms of $I$: 
\begin{align*}
\var(X | I) &= Inp_1(1-p_1)+(1-I)np_2(1-p_2)\\
E(X | I) &= Inp_1 + (1 - I)np_2
\end{align*}

Plugging in, we get: 
$$\var(X) = \frac{1}{2} (np_1(1 - p_1) + np_2(1 - p_2)) +\frac{1}{4}n^2(p_1 - p_2)^2$$
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{exercise}{Emails}
Emails arrive one at a time in an inbox. Let $T_n$ be the time at which the $n$th email arrives (measured on a continuous scale from some starting point in time). Suppose that
the waiting times between emails are i.i.d. $\Expo(\lambda)$, i.e., $T_1, T_2-T_1, T_3-T_2, \ldots$ are i.i.d.
$\Expo(\lambda)$. Each email is non-spam with probability $p$, and spam with probability $q = 1 - p$ (independently of the other emails and of the waiting times). Let $X$ be the time at which the first non-spam email arrives (so $X$ is a continuous r.v., with $X$ = $T_1$ if the 1st email is non-spam, $X = T_2$ if the 1st email is spam but the 2nd one isn't, etc...)

\begin{enumerate}
\item Find the mean and variance of $X$. 

\textit{Hint}: Let $N$ be the number of emails until the first non-spam (including that one), and write $X$ as a sum of $N$ terms; then condition on $N$.

\item Find the MGF of $X$ and state the named distribution (with parameters) of $X$. 

\textit{Hint}: $MGF_X(t) = E(e^{tX}) = E(E(e^{tX} | N))$
\end{enumerate}
\end{exercise}

\begin{solution}{3}
\vspace{-2em}
\begin{enumerate}
\item Let $X = X_1 + X_2 + \ldots X_N$, where $X_j$ is the time between the $(j-1)$st and the $j$th emails to arrive. These are all non-spam emails, and $N$ is a random variable representing the number of such non-spam emails that arrive before the first spam email (including that first spam). Therefore, $N -1$ represents the number of non-spam emails received before the first spam email, and each email has probability $p$ of being non-spam, so $N-1 \sim \Geom(p)$. Therefore, we can use Adam's and Eve's Laws to calculate $E(X)$ and $\var(X)$: 
$$ E(X) = E(E(X | N)) = E\left(N \cdot \frac{1}{\lambda} \right) = \frac{1}{\lambda} \cdot \left(\frac{q}{p} + 1 \right) = \frac{1}{\lambda p}$$  

And the variance: 
\begin{align*}
\var(X) &= E(\var(X | N)) + \var(E(X | N)) \\ 
&= E\left( N \cdot \frac{1}{\lambda}^2 \right) + \var\left(N \cdot \frac{1}{\lambda} \right)\\ 
&= \frac{1}{\lambda^2} \cdot E(N) + \frac{1}{\lambda^2} \var(N) \\ 
&= \frac{1}{\lambda^2} \left( \frac{1}{p} + \frac{q}{p^2} \right) \\ 
&= \frac{1}{\lambda^2 p^2}
\end{align*}
\item We know that the $X_i$'s are i.i.d., and thus the MGF of $X|N$ is simply the product of the MGF's of $X_i | N$. 
\begin{align*}
E\left(e^{tX}\right) &= E\left(E\left(e^{tX_1}e^{tX_2} \cdots e^t{X_N}|N\right)\right) \\ 
&= E\left( E(e^{tX_1} | N)E(e^{tX_2} | N) \cdots E(e^{tX_N} | N)  \right) \\ 
&= E\left( \pfrac{\lambda}{\lambda - t}^N \right)
\end{align*}

For convenience, let $M(t) = \frac{\lambda}{\lambda - t}$. Then, using LOTUS, we get: 

\begin{align*}
&  E\left( \pfrac{\lambda}{\lambda - t}^N \right) \\ &=\sum_{n = 1}^\infty M(t)^n \cdot P(N = n) \\ &= \sum_{n = 1}^\infty pq^{n-1} M(t)^n \\ &= 
\frac{p}{q} \sum_{n = 1}^\infty (q M(t))^n \\ 
&=\frac{p}{q} \cdot \frac{qM(t)}{1 - qM(t)} \\ 
&=\frac{p}{q} \cdot \frac{q \cdot \frac{\lambda}{\lambda - t}}{ 1  - q \cdot \frac{\lambda}{\lambda - t}} \\ 
&=\frac{p\lambda}{p \lambda - t}
\end{align*}

The sum was removed above by the sum of an infinite geometric series.

This MGF gives us: $X \sim \Expo(p \lambda)$, which agrees with the mean and variance we found above!  

\end{enumerate}
\end{solution}

\end{comment}

\begin{exercise}{Sample Mean}
For i.i.d. r.v.s. $X_1, X_2, \ldots X_n$ with mean $\mu$ and variance $\sigma^2$, give a value of $n$ (a specific number) that will ensure that there is at least a 99\% chance that the sample mean will be within 2 standard deviations of the true mean $\mu$. 
\end{exercise}

\begin{solution}{3}
Let $\bar{X}$ be the sample mean, which is equal to $\frac{X_1 + \ldots + X_n}{n}$. Saying that there is at least a 99\% chance that $|\bar{X}_n - \mu| < 2 \sigma$ is the same as saying that there is at most a 1\% chance for $|\bar{X}_n - \mu| > 2 \sigma$. Thus, we want to calculate an $n$ such that: 
$$P\left( |\bar{X}_n - \mu | > 2 \sigma \right) < 0.01$$
Applying Chebyshev's inequality, we get the following: 
$$P(|\bar{X}_n - \mu| > 2 \sigma) \leq \frac{\var \bar{X}_n}{(2 \sigma)^2} = \frac{\frac{\sigma^2}{n}}{4 \sigma^2} = \frac{1}{4n}$$

Therefore, if we choose $n = 25$, we get the desired inequality. 
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Practicing Inequalities}
Fill each inequality below with either $=, \le, \ge$ or $?$. and explain. In all instances below, assume that $X$ and $Y$ are positive random variables, although not necessarily independent. Assume that the expected values exist. \\ \\
\begin{enumerate} \itemsep 1.13cm
	\item $E(X^4) \ \underline{\hspace{1cm}} \ \sqrt{E(X^2)E(X^6)}$
	\item $P(|X+Y| > 2)  \ \underline{\hspace{1cm}} \ \frac{1}{16}E((X+Y)^4)$
	\item $\sqrt{E(X)+50}  \ \underline{\hspace{1cm}} \  E(\sqrt{X+50})$
	\item $E(Y|10X)  \ \underline{\hspace{1cm}} \  E(Y|X)$
	\item $E(\cos(X))  \ \underline{\hspace{1cm}} \  \cos(E(X))$
	\item $SD(X) + SD(Y) \ \underline{\hspace{1cm}} \ SD(X+Y)$
\end{enumerate}
\end{exercise}

\begin{solution}{0.2}
\vspace{-1.5em}
\begin{enumerate}
	\item $E(X^4) \ \boxed{\leq} \ \sqrt{E(X^2)E(X^6)}$ \\
	Note that by Cauchy-Schwarz inequality, we have that
	$$E(X^4)^2 = E(XX^3)^2 \leq E(X^2)E(X^6)$$
	\item $P(|X+Y| > 2)  \ \boxed{\leq} \ \frac{1}{16}E((X+Y)^4)$ \\
	By the Markov Inequality, we have that
	$$P(|X + Y| > 2) = P((X + Y)^4 > 2^4) \leq \frac{1}{2^4} E((X + Y)^4) = \frac{1}{16} E((X + Y)^4)$$
	\item $\sqrt{E(X)+50}  \ \boxed{\geq} \  E(\sqrt{X+50})$ \\
	By Jensen's inequality, because the square root function is a concave function, it follows that first applying the square root function and then taking an expectation yields a small value than first taking an expectation and then taking the square root.
	\item $E(Y|10X)  \ \boxed{=} \  E(Y|X)$ \\
	Knowing 10X and X is being given the exact same information, so both should definitely be equal! As a note, try not to confuse yourself and believe that being given 10X increases the value of X at all. You would literally be given the quantity 10X, and from that you can obtain X.
	\item $E(\cos(X))  \ \boxed{?} \  \cos(E(X))$ \\
	Because the cosine function is neither concave nor convex, there is no definitive answer.
	\item $SD(X) + SD(Y) \ \boxed{\geq} \ SD(X+Y)$ \\
	If we square both sides, note that we have
	$$\var(X) + \var(Y) + 2 \sqrt{\var(X) \var(Y)} \,  \underline{\hspace{0.5cm}} \, \var(X + Y)$$
	But note that $\var(X + Y) = \var(X) + \var(Y) + 2 \cov(X, Y)$, and so we have
	$$\var(X) + \var(Y) + 2 \sqrt{\var(X) \var(Y)} \,  \underline{\hspace{0.5cm}} \, \var(X) + \var(Y) + 2 \cov(X, Y)$$
	And in this case, note that $\sqrt{\var(X) \var(Y)} \geq \cov(X, Y)$ since correlation is always less than or equal to 1, and so it follows that our answer is $\geq$.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{Gamma CLT}
\end{exercise}
\begin{enumerate}
\vspace{-1.5em}
\item Explain why a Gamma random variable with parameters $(n, \lambda)$ is approximately Normal when n is large.

\item Let $X_n \sim \Gam(n, \lambda)$. Determine $a$ and $b$ such that 
$$ \frac{X_n - a}{b} \to N(0,1)$$
as $n \to \infty$
\end{enumerate}

\begin{solution}{2.5}
\vspace{-1em}
\begin{enumerate}
\item Let $X_n = Y_1 + Y_2 + \ldots + Y_n$. Then, we can have $X_n \sim \Gam(n, \lambda)$ and $Y_i$'s be i.i.d. $\Expo(\lambda)$. By the central limit theorem, since $X_n$ is the sum of i.i.d. random variables, it converges to a normal distribution as $n \to \infty$. 
\item We have reasoned above that $X_n$ is approximately normal, but what are its parameters? The central limit theorem also tells us that $$ X_n \sim N\left(\frac{n}{\lambda}, \frac{n}{\lambda^2}  \right)$$

In order to convert this normal distribution to a standard normal ($N(0,1)$), all we need to do is subtract the mean and divide by the standard deviation. Thus: 

$$ \frac{X_n - \frac{n}{\lambda}}{\frac{\sqrt{n}}{\lambda}} \sim N(0,1)$$

as $n \to \infty$. 
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{exercise}{Variance of a $t$ distribution}
% For $T \sim t_n$, $n > 2$, show that $\var(T) = \frac{n}{n - 2}$. Recall from class that for $G \sim \Gam(\alpha, \beta)$, $E(G^k) = \beta^{-k}\frac{\Gamma(\alpha + k)}{\Gamma(\alpha)}$ for $k > -\alpha$, that $\chi^2_n \sim \Gam\paren{\frac{n}{2}, \frac{1}{2}}$, and that $t_n \sim \frac{Z}{\sqrt{V / n}}$ for $Z \sim \N(0, 1)$ and $V \sim \chi^2_n$ independent.
% \end{exercise}

% \begin{solution}{3.5}
% First, we find the mean of $T$:
% \begin{align*}
% E(T) &= E\paren{\frac{Z}{\sqrt{V / n}}} \\
% &= E\paren{Z} E\paren{\frac{1}{\sqrt{V / n}}} & \textrm{by independence of $Z$ and $V$} \\
% &= 0
% \end{align*}
% Alternatively, we can reason that because the $t$ distribution is a symmetric distribution, $E(T) = 0$. Hence, $\var(T) = E(T^2)$.
% \begin{align*}
% E(T^2) &= E\paren{\paren{\frac{Z}{\sqrt{V / n}}}^2} \\
% &= E\paren{\frac{n Z^2}{V}} \\
% &= n E\paren{Z^2} E\paren{\frac{1}{V}} & \textrm{by independence of $Z$ and $V$} \\
% \end{align*}
% We have $E(Z^2) = \var(Z) = 1$ and $E(\frac{1}{V}) = E(V^{-1}) = \frac{1}{2} \frac{\Gamma(n / 2 - 1)}{\Gamma(n / 2)}$ since $V \sim \chi^2_n \sim \Gam\paren{\frac{n}{2}, \frac{1}{2}}$.
% \begin{align*}
% E(T^2) &= n \frac{1}{2} \frac{\Gamma(n / 2 - 1)}{\Gamma(n / 2)} \\
% &= n \frac{1}{2} \frac{\Gamma(n / 2 - 1)}{\paren{n / 2 - 1} \Gamma(n / 2 - 1)} \\
% &= \frac{1}{2} \frac{n}{n / 2 - 1} \\
% &= \frac{n}{n - 2}
% \end{align*}
% \end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Two-State Markov Chain}
Suppose $X_n$ is a two-state Markov chain with transition matrix

\[
Q = \bordermatrix{~ & 0 & 1 \cr
                  0 & 1-\alpha & \alpha \cr
                  1 & \beta & 1-\beta \cr}
\]
\begin{enumerate}
	\item Find the stationary distribution $\vec{s} = (s_0, s_1)$ of $X_n$ by solving $\vec{s} Q = \vec{s}$. 
	\item Show that this Markov Chain is reversible under the stationary distribution found in part (a)
	\item Let $Z_n = (X_{n-1}, X_n)$. Is $Z_n$ a Markov chain? If so, what are the states and transition matrix?
\end{enumerate}
\end{exercise}

\begin{solution}{5}
\begin{enumerate}
\item By solving $\vec{s} Q = \vec{s}$, we have that
$$s_0 = s_0 (1 - \alpha) + s_1 \beta \textrm{ and } s_1 = s_0 \alpha + s_1 (1 - \beta)$$
And by solving this system of linear equations, it follows that $\vec{s} = \paren{\frac{\beta}{\alpha + \beta}, \frac{\alpha}{\alpha + \beta}}$.

\item To verify the validity of a stationary distribution for a chain, we just need to show that $s_i q_{ij} = s_j q_{ji}$, which is done if we can show that $s_0 q_{01} = s_1 q_{10}$. We have that
$$s_0 q_{01} = \frac{\alpha\beta}{\alpha + \beta} = s_1 q_{10}$$
which satisfies our reversibility condition and verifies our stationary distribution from part (a).

\item Yes, $Z_n$ is a Markov Chain because conditional on $Z_n$, $Z_{n+1} \indep Z_{n-1}$. This is because the components of $Z_{n+1}$ and $Z{n-1}$ are either constants conditioned in $Z_n$ or independent of each other given that $X_n$ is a Markov Chain.

The states are given as $\{ (0, 0), (0, 1), (1, 0), (1, 1) \}$. The transition matrix is given as
\[
Q = \bordermatrix{   ~   & (0, 0) & (0, 1) & (1, 0) & (1, 1) \cr
                  (0, 0) & 1-\alpha & \alpha & 0 & 0 \cr
                  (0, 1) & 0 & 0 & \beta & 1-\beta \cr
                  (1, 0) & 1-\alpha & \alpha & 0 & 0 \cr
                  (1, 1) & 0 & 0 & \beta & 1-\beta \cr
                 }
\]
\end{enumerate}
\end{solution}

\pagebreak

\begin{exercise}{Symmetrical Chain} 
A Markov chain $X_0,X_1, X_2\ldots$ with state space $\{3,2,1,0,1,2, 3\}$ proceeds as follows. The chain starts at $X_0 = 0$. If $X_n$ is not an endpoint ($-3$ or $3$), then $X_{n+1}$ is
$X_n +1$ or $X_n -1$, each with probability $1/2$. Otherwise, the chain gets reflected off the endpoint, i.e., from 3 it always goes to 2 and from $-3$ it always goes to $-2$. A diagram
of the chain is shown below.\begin{center}
\includegraphics[scale=0.5]{markov1.png}
\end{center}
\begin{enumerate}
\item Is $|X_0|, |X_1|, |X_2|, \ldots$ a Markov Chain? 

\item Define the sign function $S(x)$ as follows: 
\[ S(x) = \begin{cases} 
      1 & x > 0 \\
      0 & x = 0 \\
      -1 & x < 0
   \end{cases}
\]

Is $S(X_0), S(X_1), S(X_2), \ldots$ a Markov Chain? 

\item Find the stationary distribution of the original chain: $X_1, X_2, X_3 \ldots$

\item Find a simple way to modify some of the transition probabilities $q_{ij}$ for $i,j \in \{-3, 3\}$ to make the stationary distribution of the modified chain uniform over the states.
\end{enumerate}

\end{exercise}


\begin{solution}{5.5} 
\vspace{-0.5em}
\begin{enumerate}
\item Yes it is a Markov chain with state space $\{0, 1, 2, 3\}$. We need to check that this new chain satisfies the Markov property. If we are given that $|X_n| = 1$, then regardless of what $|X_{n-1}|, |X_{n-2}|, \ldots$ are, we know that there is a $1/2$ chance $|X_{n+1}|$ will be 0 and a $1/2$ chance that it will be $2$. A similar argument can be made for all the states, and thus this chain satisfies the Markov property. 

\item We will show that this is not a Markov chain because it does not satisfy the Markov property. In general, when showing that something does or does not satisfy the Markov property, it is a good idea to ask yourself: "If I knew more than just the \textit{one} previous state, would that help me make a more informed decision about what the next state is?". 

In this case, if I told you that the last three states $S(X_{n-2}), S(X_{n-1}), S(X_n)$ were $0,1,1$, then we know that the chain is at $2$, i.e. $X_n = 2$. This means that $P(S(X_{n+1}) = 0) = 0$ because we can't jump from 2 to 0. However, if I told you that the last two states $S(X_{n-1}), S(X_n)$ were $0$ and $1$, then you would know that $X_n = 1$ and say that $P(S(X_{n+1}) = 0) = 1/2$. Thus, the Markov property is not satisfied because even though in both situations $S(X_n) = 1$, we had different probabilities for $P(S(X_{n+1}) = 1)$. 

\item We see that this is simply a random walk on an undirected graph, so the stationary distribution is proportional to the degrees of each of the states. The degrees are: $(1, 2, 2, 2, 2, 2, 1)$, and so the stationary distribution is: $\vec{s} = (1/12, 1/6, 1/6, 1/6, 1/6, 1/6, 1/12)$ 

\item If we could somehow get all the states to have the same degree, then the stationary distribution would be uniform across all states. Therefore, all we need to do is connect states 3 and -3 together, so that there is a $1/2$ chance at 3 to go to $2$ or $-3$, and equivalently for $-3$. 
\end{enumerate}
\end{solution} 

\pagebreak

\begin{exercise} {Not a Markov Chain}
A Markov chain has two states, $A$ and $B$, with transitions as follows:
\begin{center}
\includegraphics[scale=0.6]{markov2.png}
\end{center}

Suppose we do not get to observe this Markov chain, which we'll call $X_1, X_2, \ldots$.  Instead, whenever the chain transitions from $A$ back to $A$, we observe a 0, and whenever it changes states, we observe a 1. Let the sequence of 0's and 1's be called $Y_0, Y_1, Y_2, \ldots $
For example, if the X chain starts out as
$$A, A, B, A, B, A, A, \ldots$$
then the Y chain starts out as
$$0, 1, 1, 1, 1, 0, \ldots$$

\begin{enumerate}
\item Show that $Y_0, Y_1, Y_2, \ldots$ is not a Markov Chain. 

\item In the past, when we have encountered processes that are not directly Markov chains, our remedy was to create a new state $Z_n$ which would represent not only the current state $X_n$ that the chain is on, but also remember the past $m$ states. Thus, $Z_n$ is not just one state, but a tuple of states that represent the chain's history: 
$$ Z_n = \left(X_n, X_{n-1}, X_{n-2} \ldots X_{n - (m-1)} \right)$$
Show that such a trick will not work for $Y_0, Y_1, Y_2\ldots$. That is, no matter how large $m$ is, $\{Z_n\}$ will never be a Markov Chain. 
\end{enumerate}
\end{exercise}

\begin{solution} {5} 
\vspace{-0.5em}
\begin{enumerate}
\item We will show that knowing the chain outputted a 1 is not enough information to give the probability of the chain outputting 1 again on the next iteration. This is true because if the most recent sequence of outputs have been $0,1$, then the chain is clearly at $B$ in which case the probability of it outputting $1$ next is 1. However, if the chain's outputs had been $0,1,1$, then we know that the chain is at state $A$ in which case the probability of the next output being 1 (and hence the next state be B) is only $1/2$. 

\item The problem is saying that no finite amount of "memory" in this Markov chain is enough to be able to give a unique set of probabilities for the next output being $0$ or $1$, given a history of $m$ outputs. 

We notice that $1$'s in this Markov chain output always come in pairs. Assuming that you start at state A (which is valid because state A is recurrent), we see that outputting a $1$ means that another $1$ must follow. 

Suppose again that we're in the situation where out last output has been $1$. From the above fact that $1$'s always come in pairs, we can reason that it's not the number of $1$'s that have been outputted in the recent past that matters, but rather its parity (i.e even or odd). If a 0 is followed by an even number of $1$'s, then we know that the chain is at state $A$ so the probability of outputting either a $1$ or $0$ is $1/2$. If the number of $1$'s following that first $0$ is odd, then we know that the next output must be $1$ with probability 1.  

No amount of memory $m$ can completely capture the parity of the number or $1$'s that came before. This doesn't mean that there doesn't exist some clever way to create states such that this sequence of $0$'s and $1$'s becomes a Markov chain. Rather, this conclusions says that this one particular technique (remembering the past $m$ states) does not work in this case. 

\end{enumerate}
\end{solution}
\end{document}
