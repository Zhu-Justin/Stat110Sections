\documentclass[11.5pt]{article}
\usepackage{stat110}

\title{Conditional Expectation}
\sectionnum{9}

\author{\shira, \tim, \creds}

\SOLUTION

\begin{document}

\maketitle

\begin{notes}
\section*{Conditional Expectation}
\begin{description}
	\item[Conditioning on an Event] - We can find the expected value of $Y$ given that event $A$ or $X=x$ has occurred. This would be finding the values of $E(Y|A)$ and $E(Y|X = x)$. Note that conditioning in an event results in a $number$. Note the similarities between regularly finding expectation and finding the conditional expectation. The expected value of a dice roll given that it is prime is $\frac{1}{3}2 + \frac{1}{3}3 + \frac{1}{3}5 = 3\frac{1}{3}$. The expected amount of time that you have to wait until the shuttle comes (assuming that the waiting time is $\sim \Expo(\frac{1}{10})$) given that you have already waited $n$ minutes, is 10 more minutes by the memoryless property.
		\begin{table}[htb!]
		   \centering
			\begin{tabular}{ccc}
			\toprule
				 ~& \textbf{Discrete Y} & \textbf{Continuous Y} \\
			\midrule
				 Conditional & $E(Y|A) = \sum_y yP(Y=y|A)$ & $E(Y|A) = \int_{-\infty}^\infty yf(y|A)dy$ \\ 
				 ~ & $E(Y|X=x) = \sum_y yP(Y=y|X=x)$ & $E(Y|X=x) =\int_{-\infty}^\infty yf_{Y|X}(y|x)dy$ \\
			\midrule
			Regular & $E(Y) = \sum_y yP(Y=y)$ & $E(Y) =\int_{-\infty}^\infty yf_Y(y)dy$ \\
			\bottomrule
			\end{tabular}
		\end{table}
	\vspace{-.45 cm}
	\item[Conditioning on a Random Variable] - We can also find the expected value of $Y$ given the random variable $X$. The resulting expectation, $E(Y|X)$ is \emph{not a number but a function of the random variable X}. For an easy way to find $E(Y|X)$, find $E(Y|X = x)$ and then plug in $X$ for all $x$. This changes the conditional expectation of $Y$ from a function of a number $x$, to a function of the random variable $X$.

\end{description}
\section*{Law of Total Expectation}
This is an extension of the \emph{Law of Total Probability}. For any set of events $B_1, B_2, B_3, ... B_n$ that partition the sample space (simplest case being $\{B, B^c\})$:
\begin{align*}
    E(X) &= \sum_{x} x P(X=x) \\
    &= \sum_{x} x \sum_{i} P(X =x| B_i) P(B_i)\\
    &= \sum_x \sum_i xP(X=x | B_i) P(B_i)\\
    &= \sum_i \sum_x xP(X=x|B_i) P(B_i)\\
    &= \sum_i E(X|B_i)P(B_i)
\end{align*}

\section*{Properties of Conditional Expectation}
\begin{description}
\item [Independence] - if $X$ and $Y$ are independent, then: 
$$ E(Y | X) = E(Y)$$ 
This is because $Y | X \sim X$ as conditioning on $X$ gives us no additional information about $Y$. 

\item [Taking out what's Known] - If we are finding the expectation that involves some function of $h(X)$ and we're conditioning on $X$, then we can treat $h(X)$ as a constant because $X$ is known. 
$$ E(h(X) Y | X) = h(X) E(Y | X)$$
 
\item [Linearity] - We have linearity in the first term: 
$$ E(aY_1 + bY_2 | X) = a E(Y_1 | X) + bE(Y_2 | X)$$

\item [Adam's Law] - For any two random variables $X,Y$: 
$$ E(E(Y | X)) = E(Y)$$
This is also called the \textbf{Law of Iterated Expectation}
Here's a short proof to convince yourself that the above is true: 
Let $g(X) = E(Y|X)$ (this is true because we said earlier that $E(Y|X)$ is a random variable in terms of $X$). Then, the left hand side becomes: 

$$ E(E(Y|X)) = E(g(X)) = \sum_{x} g(X) \cdot P(X = x) = \sum_x E(Y | X=x) \cdot P(X = x)$$

but that is simply equal to $E(Y)$ by the law of total expectation! We have considered all possible outcomes that $X$ can take on, weighted by $P(X = x)$ (how likely that outcome of $X$ is). \\

You can also do Adam's Law with extra conditioning! 
    $$E(Y |X) = E(E(Y |X, Z)|X)$$   

\item[Eve's Law] - For any two random variables $X,Y$:
    $$ \var{(Y)} = E_X( \var{(Y | X)}) + \var_X{(E(Y | X))}$$

Notice that both $\var{(Y | X)}$ and $E(Y | X)$ are functions of the random variable $X$. 
\end{description}


\end{notes} 

\newpage
\section*{Practice Problems} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}{Chicken-Egg Revisited}
Recall in the Chicken-Egg problem that we have a chicken that lays eggs, where the number of eggs $N$ follows the distribution $N\sim \Pois(\lambda)$. Given that the chicken lays $N$ eggs, the number of eggs that hatch $X$ follows the distribution $X|N \sim \Bin(N,p)$. Find $E(X)$ and $Var(X)$.
\end{exercise}

\begin{solution}{2}
Use Adam's and Eve's Law!
\begin{align*}
    E(X) &= E[E(X|N)] = E[Np] = p E[N] =\lambda p\\
    Var(X) &= E[Var(X|N)] + Var[E(X|N)] \\
    &= E[Np(1-p)] + Var[Np] \\
    &= \lambda p(1-p) + p^2 \lambda \\
    &= \lambda p
\end{align*}
\end{solution}

\begin{exercise}{Consumerism}
When customers enter a particular store, each makes a purchase with probability $p$, independently. Given
that a customer makes a purchase, the amount spent has mean $\mu$ (in dollars) and
variance $\sigma^2$. Find the mean and variance of how much a random customer spends (note that the
customer may spend nothing). 
\end{exercise}
\begin{solution}{3}
Let $X$ be the amount a random customer spends at the store, and let $I$ be the indicator that a random customer makes a purchase. We then apply the Law of Total Expectation:
\begin{align*}
    E[X] &= E[X | I= 0]P(I=0) + E[X | I=1]P(I=1) = \mu p \\
    E[X^2] &= E[X^2 | I=0]P(I=0) + E[X^2 | I=1]P(I=1) \\
    &= E[X^2 | I=1]P(I=1) \\
    &= (\mu^2 + \sigma^2)p \\
    Var[X] &= E[X^2] - E[X]^2 \\
    &= \sigma^2 p +\mu^2 p (1-p)
\end{align*}
\end{solution}

\begin{exercise}{Trapped Miners}
A miner is trapped in a mine containing 3 doors. The first door leads to a tunnel that will take him to safety after 3 minutes. The other two doors lead to tunnels that will return him to the mine after 5 and 7 minutes of travel each. The miner is equally likely to choose any of the doors at any time. What is the expected amount of time until he reaches safety?
\end{exercise}

\begin{solution}{4}
Let $W$ be the total amount of time taken until he reaches safety, and $D_1$, $D_2$, and $D_3$ be the events of going through doors 1, 2, and 3 respectively. \\
Using LoTE, we have
\begin{align*}
E(W) &= E(W|D_1)P(D_1) + E(W|D_2)P(D_2) + E(W|D_3)P(D_3) \\
&= 3 \frac{1}{3} + (5 + E(W)) \frac{1}{3} + (7 + E(W)) \frac{1}{3} \\
\frac{1}{3} E(W) &= 5 \\
E(W) &= 15
\end{align*}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%b
\begin{comment}
\begin{exercise}{Coin Tossing}
You toss four coins in the air, and you earn an amount in dollars equal to the number of coins that land heads. However, after the coins land on the table for the first time, you get to re-toss all four coins if you want. What is your best strategy, and what are your expected earnings of that strategy?
\end{exercise}

\begin{solution}{4}
Our best strategy is to re-toss if and only if the first toss results in 0, 1, or 2 heads. (For 2 heads it doesn't matter whether we re-toss or not, but let's say we do.)

Let Y be our final earnings, and we condition on X, the number of heads on the first toss. Using LoTE:

\begin{align**}
E(Y) &= E(Y | X = 0, 1, \textrm{ or } 2)P(X = 0, 1, \textrm{ or } 2) + E(Y | X = 3)P(X = 3) + E(Y | X = 4)P(X = 4) \\
&= 2 \cdot \frac{11}{16} + 3 \cdot \frac{1}{4} + 4 \cdot \frac{1}{16} \\
&= \frac{19}{8}
\end{align**}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Two Coins}
One of two identical-looking coins is picked from a hat randomly, where one coin has probability $p_1$ of Heads and the other has probability $p_2$ of Heads. Let $X$ be the number of Heads after flipping the chosen coin $n$ times. Find the mean and variance of $X$
\end{exercise}

\begin{solution}{2.5}
We use the law of total expectation to condition on which coin we selected. Let $I$ be an indicator random variable of selecting coin 1. Then we have: 

$$ E(X) = E(X | I = 1) \cdot P( I = 1) + E(X | I = 0) \cdot P(I = 0) $$
which simplifies to: 

$$ p_1 \cdot \frac{1}{2} + p_2 \cdot \frac{1}{2} = \frac{p_1 + p_2}{2}$$ 

We will use Eve's Law to calculate the variance: 
\begin{align**}
\var(X) &= E( \var{(X| I)}) + \var{(E(X| I))} \\ 
\end{align**}
Now, we write $\var(X | I)$ and $E(X | I)$ in terms of $I$: 

\begin{align**}
\var(X | I) &= Inp_1(1-p_1)+(1-I)np_2(1-p_2)\\
E(X | I) &= Inp_1 + (1 - I)np_2
\end{align**}

Plugging in, we get: 

$$\var(X) = \frac{1}{2} (np_1(1 - p_1) + np_2(1 - p_2)) +\frac{1}{4}n^2(p_1 - p_2)^2$$

\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Emails}
Emails arrive one at a time in an inbox. Let $T_n$ be the time at which the $n$th email arrives (measured on a continuous scale from some starting point in time). Suppose that
the waiting times between emails are i.i.d. $\Expo(\lambda)$, i.e., $T_1, T_2-T_1, T_3-T_2, \ldots$ are i.i.d.
$\Expo(\lambda)$. Each email is non-spam with probability $p$, and spam with probability $q = 1 - p$ (independently of the other emails and of the waiting times). Let $X$ be the time at which the first non-spam email arrives (so $X$ is a continuous r.v., with $X$ = $T_1$ if the 1st email is non-spam, $X = T_2$ if the 1st email is spam but the 2nd one isn't, etc...)

\begin{enumerate}
\item Find the mean and variance of $X$. 

\textit{Hint}: Let $N$ be the number of emails until the first non-spam (including that one), and write $X$ as a sum of $N$ terms; then condition on $N$.

\item Find the MGF of $X$ and state the named distribution (with parameters) of $X$. 

\textit{Hint}: $MGF_X(t) = E(e^{tX}) = E(E(e^{tX} | N))$
\end{enumerate}
\end{exercise}

\begin{solution}{4}
\begin{enumerate}
\vspace{-1em}
\item 
Let $X = X_1 + X_2 + \ldots X_N$, where $X_j$ is the time between the $(j-1)$st and the $j$th emails to arrive. These are all non-spam emails, and $N$ is a random variable representing the number of such non-spam emails that arrive before the first spam email (including that first spam). Therefore, $N -1$ represents the number of non-spam emails received before the first spam email, and each email has probability $p$ of being non-spam, so $N-1 \sim \Geom(p)$. Therefore, we can use Adam's and Eve's Laws to calculate $E(X)$ and $\var(X)$: 

$$ E(X) = E(E(X | N)) = E\left(N \cdot \frac{1}{\lambda} \right) = \frac{1}{\lambda} \cdot \left(\frac{q}{p} + 1 \right) = \frac{1}{\lambda p}$$  

And the variance: 
\begin{align**}
\var(X) &= E(\var(X | N)) + \var(E(X | N)) \\ 
&= E\left( N \cdot \frac{1}{\lambda}^2 \right) + \var\left(N \cdot \frac{1}{\lambda} \right)\\ 
&= \frac{1}{\lambda^2} \cdot E(N) + \frac{1}{\lambda^2} \var(N) \\ 
&= \frac{1}{\lambda^2} \left( \frac{1}{p} + \frac{q}{p^2} \right) \\ 
&= \frac{1}{\lambda^2 p^2}
\end{align**}
\item We know that the $X_i$'s are i.i.d., and thus the MGF of $X|N$ is simply the product of the MGF's of $X_i | N$. 
\begin{align**}
E\left(e^{tX}\right) &= E\left(E\left(e^{tX_1}e^{tX_2} \cdots e^t{X_N}|N\right)\right) \\ 
&= E\left( E(e^{tX_1} | N)E(e^{tX_2} | N) \cdots E(e^{tX_N} | N)  \right) \\ 
&= E\left( \pfrac{\lambda}{\lambda - t}^N \right)
\end{align**}

For convenience, let $M(t) = \frac{\lambda}{\lambda - t}$. Then, using LOTUS, we get: 

\begin{align**}
&  E\left( \pfrac{\lambda}{\lambda - t}^N \right) \\ &=\sum_{n = 1}^\infty M(t)^n \cdot P(N = n) \\ &= \sum_{n = 1}^\infty pq^{n-1} M(t)^n \\ &= 
\frac{p}{q} \sum_{n = 1}^\infty (q M(t))^n \\ 
&=\frac{p}{q} \cdot \frac{qM(t)}{1 - qM(t)} \\ 
&=\frac{p}{q} \cdot \frac{q \cdot \frac{\lambda}{\lambda - t}}{ 1  - q \cdot \frac{\lambda}{\lambda - t}} \\ 
&=\frac{p\lambda}{p \lambda - t}
\end{align**}

The sum was removed above by the sum of an infinite geometric series.

This MGF gives us: $X \sim \Expo(p \lambda)$, which agrees with the mean and variance we found above!  

\end{enumerate}
\end{solution}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
