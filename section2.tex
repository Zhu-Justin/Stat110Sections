\documentclass[11pt]{article}
\usepackage{stat110}
\usepackage{textcomp}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\title{Conditional Probability}
\sectionnum{2}

\author{\justin}

%\SOLUTION

\begin{document}

\maketitle

\begin{notes}

\section*{Conditional Probability}
\begin{description}
      \item[Reading Right to Left] - We read "|" right to left, that is, $A|B$ means B occurred first, then A
    \item[Conditional Probability] - Suppose we observe event $B$ and are interested in the probability of event $A$ occurring given this information. Then,
        \[P(A|B) = \frac{P(A \cap B)}{P(B)}\]
    \item[Bayes' Rule] - This is arguably one of the most important concepts and tools you will
    learn in this course. 
        \[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]
\end{description}

\subsection*{Bridging Conditional Probability and Sets}

An intuitive way to visualize conditional probability is to think about the intersection of sets.  In order to find the intersection of two different sets ${A}$ and ${B}$, we establish one of these sets to be our sample space and find the likely occurrence of the other set within this established sample space.
       $$P(A\cap B) = P(B|A)P(A) = P(A|B)P(B)$$
$$P(A_1\cap A_2\cap A_3\cap\cdots A_n) = P(A_1|A_2\cap A_3\cap\cdots A_n)P(A_2| A_3\cap\cdots A_n)\cdots P(A_{n-1}|A_n)P(A_n)$$

\subsection*{Law of Total Probability (LOTP)}
A common theme in this course is that it is far easier to solve a problem by breaking it down into smaller, simpler components than tackling it head-on. LOTP is one such tool. Suppose you want to find the probability of some event $B$, and you can partition the sample space into disjoint events $A_1, A_2, \dots, A_n$. Then, 
\begin{align}
    P(B) &= \sum_{i=1}^n P(B | A_i) P(A_i)\\
    &= \sum_{i=1}^n P(B \cap A_i)
\end{align}

We often use LOTP with Bayes' rule! Specifically, the denominator of Bayes' rule, $P(B)$, is often difficult to calculate outright, so we will instead calculate it in terms of LOTP. 

\subsection*{Extra Conditioning}
Incorporating extra information $C$ is a simple extension of Bayes' rule and LOTP:
\begin{align}
    P(A|B, C) &= \frac{P(A \cap B | C)}{P(B|C)} \\
    &= \frac{P(B|A,C)P(A|C)}{P(B|C)}\\
    P(B|C) &= \sum_{i=1}^n P(B | A_i, C) P (A_i |C)\\
    &= \sum_{i=1}^n P(B \cap A_i |C)
\end{align}

\subsection*{Disjoint vs. Independent}
Disjoint, or mutually exclusive, events are events that cannot occur simultaneously. That is, observing event $A$ precludes the possibility of also observing event $B$. We can state this equivalently as 
    \[P(A \cap B) = 0\]
Independent events are events such that observing event $B$ yields no information about the possibility of also observing event $A$. That is, conditioning on observing event $B$, the probability of observing  event $A$ is unchanged.
    \[P(A|B) = P(A)\]
We can apply this result to Bayes' rule and quickly demonstrate an alternative definition of independence:
    \[P(A \cap B) = P(A)P(B)\]

Another form of independence is conditional independence. Two events $A$ and $B$ are said to be conditionally independent given $C$ if 
    \[P(A \cap B | C) = P(A|C) P(B|C)\]
However, just as pairwise independence does not imply independence (and vice versa), conditional independence does not imply independence (and vice versa).
\end{notes}
\newpage
       

\section*{Practice Problems and Brainteasers}

% Literally just a SIG interview question reworded to reflect the current season LOL
\begin{exercise}{Recruiting}
Suppose the Harvard Consulting, Investment, and Tech Group\texttrademark  currently consists of two freshmen and some number of upperclassmen. A new student joins the group, but she forgot to indicate what year she was in! At the next club meeting, a recruiter from BainBookSachs\texttrademark  comes in and plucks a lucky student to join their ranks. Given that the student is a freshman, what is the probability that the student that just joined was a freshman? Suppose that freshmen and upperclassmen are equally likely to join HCITG. 

\begin{solution}{2}
Let there be $u$ upperclassmen in the group, $A$ be the event that the new student was a freshman, and $B$ be the probability that the selected student was the freshman. We are clearly interested in 
 \[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]
Here, $P(B|A) = \frac{3}{3+u}$, as we are conditioning that a freshman joined the group. Next, we can calculate $P(B)$ using LOTP. 
    \[P(B) = P(B|A)P(A) + P\left(B|A^c\right)P\left(A^c\right) = \frac{1}{2}\cdot\frac{3}{3+u} + \frac{1}{2}\cdot  \frac{2}{3+u}\]
Plugging everything in, we find that $P(A|B) = \frac{3}{5}$
\end{solution}
\end{exercise}


\begin{exercise}{Russian Roulette}
In a game of Russian Roulette, you find a revolver with \textbf{six} chambers containing two real bullets \textbf{side-by-side} and four empty chambers. You spin the chamber and point the gun at yourself... Click. No bullet.  It is your turn again, but do you want to spin the barrel again or just pull the trigger?  

\begin{solution}{2}
The probability that you are shot if you spin the barrel again is simply $\frac{1}{3}$, as there are still two bullets and four empty chambers. To calculate the probability of being shot if the interviewer pulls the trigger right away, we can use conditional probability! Let $B$ be the event that the previous shot was not a bullet, and let $A$ be the event that the next adjacent shot in the chamber is a bullet. We know that $P(B) = \frac{2}{3}$ and $P(A \cap B) = \frac{1}{6}$, as there is only one way for the previous shot to not be a bullet while the next shot is a bullet. Therefore, 
    $$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1}{4}$$
So you're better off taking the next shot without spinning the chamber!     
\end{solution}

\end{exercise}
\end{document}
