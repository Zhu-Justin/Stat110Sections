\documentclass[11.5pt]{article}
\usepackage{stat110}

\title{Transformations, Beta, Gamma, Order Statistics}
\sectionnum{8}

\author{\justin}

%\SOLUTION

\begin{document}

\maketitle

\begin{notes}

\section*{Continuous Transformations}
\begin{description}
	\item[Why do we need the Jacobian?] We need the Jacobian to rescale our PDF so that it integrates to 1.
	\item[One Variable Transformations] Let's say that we have a random variable $X$ with PDF $f_X(x)$, but we are also interested in some function of $X$. We call this function $Y = g(X)$. Note that $Y$ is a random variable as well. If $g$ is differentiable and one-to-one (every value of $X$ gets mapped to a unique value of $Y$), then the following is true:
	\[f_Y(y) = f_X(x)\left|\frac{dx}{dy}\right| \textnormal{ or } f_Y(y) \left|\frac{dy}{dx}\right|= f_X(x)\]
	To find $f_Y(y)$ as a function of $y$, plug in $x = g^{-1}(y)$.
	\[f_Y(y) = f_X(g^{-1}(y))\left|\frac{d}{dy}g^{-1}(y)\right|\]
	The derivative of the inverse transformation is referred to the \textbf{Jacobian}, denoted as $J$.
	\[J = \frac{d}{dy}g^{-1}(y)\]
	\item[Two Variable Transformations] Similarily, let's say we know the joint distribution of $U$ and $V$ but are also interested in the random vector $(X, Y)$ found by $(X, Y) = g(U, V)$. If $g$ is differentiable and one-to-one, then the following is true:
	\[f_{X,Y}(x, y) = f_{U,V}(u,v) \left|\left| \frac{\delta(u, v)}{\delta(x, y)} \right|\right| = f_{U,V}(u,v)\left| \left| 
	\begin{array}{ccc}
		\frac{\delta u}{\delta x} & \frac{\delta u}{\delta y} \\
		\frac{\delta v}{\delta x} & \frac{\delta v}{\delta y} 
	\end{array}
	\right| \right| \textnormal{ or } f_{X,Y}(x, y) \left|\left| \frac{\delta(x, y)}{\delta(u, v)} \right|\right| = f_{U,V}(u,v) 
	\]
	The outer $||$ signs around our matrix tells us to take the absolute value. The inner $||$ signs tells us to the matrix's determinant. Thus the two pairs of $||$ signs tell us to take the absolute value of the determinant matrix of partial derivatives. In a 2x2 matrix, 
	\[ \left| \left|
	\begin{array}{ccc}
		a & b \\
		c & d
	\end{array}
	\right| \right| = |ad - bc|\]
	The determinant of the matrix of partial derivatives is referred to the \textbf{Jacobian}, denoted as $J$.
	\[\left| \begin{array}{ccc}
		\frac{\delta u}{\delta x} & \frac{\delta u}{\delta y} \\
		\frac{\delta v}{\delta x} & \frac{\delta v}{\delta y} 
	\end{array}\right| = J\]

\end{description}

\newpage

\section*{Gamma Function}
\begin{description}
\item[The Letter Gamma] - $\Gamma$ is the (capital) roman letter Gamma. It is used in statistics for both the Gamma function and the Gamma Distribution.
\item[Recursive Definition] - The Gamma function is an extension of the factorial function to all real (and complex) numbers, with the argument shifted down by 1. When $n$ is a positive integer, 
	\[\Gamma(n) = (n-1)!\]
For all values of $n$ (except -1 and 0),
	\[\Gamma(n + 1) = n\Gamma(n)\]
\item[Closed-form Definition] - The Gamma function is defined as:
	\[\Gamma(n) = \int_0^\infty t^{n-1}e^{-t}dt\]
%\item[Example Values]
\end{description}
%	\begin{center}
%		\begin{tabular}{lll}
%			$\Gamma(1) = 0! = 1$ & $\Gamma(2) = 1! = 1$ & $\Gamma(3) = 2! = 2$ \\
%			$\Gamma(\frac{1}{2}) = \sqrt{\pi}$ & $\Gamma(\frac{3}{2}) = \frac{\pi}{2}$  & $\Gamma(\frac{5}{2}) = \frac{3\sqrt{\pi}}{4}$ \\
%		\end{tabular}
%	\end{center}

\section*{Gamma Distribution (Continuous)}
Let us say that $X$ is distributed $\Gam(a, \lambda)$. We know the following:
\begin{description}
	\item[Story] You sit waiting for shooting stars, and you know that the waiting time for a star is distributed $\Expo(\lambda)$. You want to see ``$a$" shooting stars before you go home. $X$ is the total waiting time for the $a$th shooting star.
	\item[Example]	You are at a bank, and there are 3 people ahead of you. The serving time for each person is distributed Exponentially with mean of 2 time units. The distribution of your waiting time until you begin service is $\Gam(3, \frac{1}{2})$
	\item[PDF] The PDF of a Gamma is:
\begin{eqnarray*}
f(x) = \frac{1}{\Gamma(a)}(\lambda x)^ae^{-\lambda x}\frac{1}{x},
\hspace{.1 in}
x \in [0, \infty)
\end{eqnarray*}
	\item[Properties and Representations]
\end{description}
\vspace{-.4 cm}
	\begin{eqnarray*}
		E(X) = \frac{a}{\lambda} && Var(X) = \frac{a}{\lambda^2} \\
		X \sim \Gam(a, \lambda), \hspace{.2 cm} Y \sim \Gam(b, \lambda), \hspace{.2cm} X \independent Y &\longrightarrow& X + Y \sim \Gam(a + b, \lambda), \hspace{.3cm} \frac{X}{X + Y} \independent X + Y \\
		X \sim \Gam(a, \lambda) &\longrightarrow& X = X_1 + X_2 + ... X_a \textnormal{ for $X_i$ i.i.d. $\Expo(\lambda)$} \\
		\Gam(1, \lambda) &\sim& \Expo(\lambda) 
	\end{eqnarray*}

\pagebreak

\section*{Beta Distribution (Continuous)}
Let us say that $X$ is distributed $\Beta(a, b)$. We know the following:
\begin{description}
	\item[Story (Bank/Post-Office)] Let's say that your waiting time at the bank is distributed $X \sim \Gam(a, \lambda)$ and that your total waiting time at the post-office is distributed $Y \sim \Gam(b, \lambda)$. You visit both of them while doing errands. Your total waiting time at both is $X + Y \sim \Gam(a + b, \lambda)$ and the fraction of your time that you spend waiting at the Bank is $\frac{X}{X+Y} \sim \Beta(a, b)$. The fraction is not dependent on $\lambda$, and $\frac{X}{X+Y} \independent X + Y$.
	\item[Example] You are tasked with finding Jules, Vernes, and Nemo in a friendly game of hide and seek. You look for them in order, and the time it takes to find one of them is distributed Exponentially with a mean of $\frac{1}{3}$ of a time unit. The time it takes to find both Jules and Vernes is $\Expo(3) + \Expo(3) \sim \Gam(2, 3)$. The time it takes to find Nemo is $\Expo(3) \sim \Gam(1, 3)$. Thus the proportion of the total hide-and-seek time that you spend finding Nemo is distributed $\Beta(1, 2)$ and is independent from the total time that you've spent playing the game.
	\item[PDF] The PDF of a Beta is:
		\begin{eqnarray*}
		f(x) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1},
		\hspace{.1 in}
		x \in (0, 1)
		\end{eqnarray*}
	\item[Properties and Representations]
\end{description}
\vspace{-.4 cm}
	\begin{eqnarray*}
		E(X) &=& \frac{a}{a + b} \\
		X \sim \Gam(a, \lambda), \hspace{.3cm} Y \sim \Gam(b, \lambda), \hspace{.3cm} X\independent Y \hspace{.3cm} &\longrightarrow& \hspace{.3cm} \frac{X}{X + Y} \sim \Beta(a, b), \hspace{.3cm} \frac{X}{X + Y} \independent X + Y \\
		\Beta(1, 1) &\sim& \Unif(0, 1)
	\end{eqnarray*}

\section*{Notable Uses of the Beta Distribution}
\begin{description}
	\item[\dots as the Order Statistics of the Uniform] -  The smallest of three Uniforms is distributed $U_{(1)} \sim \Beta(1, 3)$. The middle of three Uniforms is distributed $U_{(2)} \sim \Beta(2, 2)$, and the largest $U_{(3)} \sim \Beta(3, 1)$. The distribution of the the $j^{th}$ order statistic of $n$ i.i.d Uniforms is:
	\begin{align*}
		U_{(j)} &\sim \Beta(j, n - j + 1) \\
		f_{U_{(j)}}(u) &= \frac{n!}{(j-1)!(n-j)!}t^{j-1}(1-t)^{n-j}
	\end{align*}
	
	
	\item[\dots as the Conjugate Prior of the Binomial] - A prior is the distribution of a parameter before you observe any data ($f(x)$). A posterior is the distribution of a parameter after you observe data $y$ ($f(x|y)$). Beta is the \emph{conjugate} prior of the Binomial because if you have a Beta-distributed prior on $p$ (the parameter of the Binomial), then the posterior distribution on $p$ given observed data is also Beta-distributed. This means, that in a two-level model:
	\begin{align*}
		X|p &\sim \Bin(n, p) \\
		p &\sim \Beta(a, b)
	\end{align*}
Then after observing the value $X = x$, we get a posterior distribution $p|(X=x) \sim \Beta(a + x, b + n - x)$
\end{description}

\newpage

\section*{Special Cases of Beta and Gamma}
\[\Gam(1, \lambda) \sim \Expo(\lambda) \hspace{1 cm} \Beta(1, 1) \sim \Unif(0, 1)\]

\section*{Bank and Post Office Result}
Let us say that we have $X \sim \Gam(a, \lambda)$ and $Y \sim \Gam(b, \lambda)$, and that $X \independent Y$. By Bank-Post Office result, we have that:
\begin{align*}
	X + Y &\sim \Gam(a + b, \lambda)\\
	\frac{X}{X + Y} &\sim \Beta(a, b)\\
	X + Y &\independent \frac{X}{X + Y}
\end{align*}



\section*{Order Statistics}
\begin{description}
	\item[Definition] - Let's say you have $n$ i.i.d. random variables $X_1, X_2, X_3, \dots X_n$. If you arrange them from smallest to largest, the $i$th element in that list is the $i$th order statistic, denoted $X_{(i)}$. $X_{(1)}$ is the smallest out of the set of random variables, and $X_{(n)}$ is the largest.
	\item[Properties] - The order statistics are dependent random variables. The smallest value in a set of random variables will always vary and itself has a distribution. For any value of $X_{(i)}$, $X_{(i+1)} \geq X_{(j)}$.
	\item[Distribution] - Taking $n$ i.i.d. random variables $X_1, X_2, X_3, \dots X_n$ with CDF $F(x)$ and PDF $f(x)$, the CDF and PDF of $X_{(i)}$ are as follows:
	\begin{align*}
		F_{X_{(i)}}(x) = P (X_{(i)} \leq x) &= \sum_{k=i}^n {n \choose k} F(x)^k(1 - F(x))^{n - k} \\
		f_{X_{(i)}}(x) &= n{n - 1 \choose i - 1}F(x)^{i-1}(1 - F(X))^{n-i}f(x)
	\end{align*}
	\item[Universality of the Uniform] - We can also express the distribution of the order statistics of $n$  i.i.d. random variables $X_1, X_2, X_3, \dots X_n$ in terms of the order statistics of $n$ uniforms. We have that
	\[F(X_{(j)}) \sim U_{(j)}\]
\end{description}


\end{notes}

\newpage

\section*{Practice Problems} 


\begin{exercise}{Transformations of the Uniform}
Let $U \sim \Unif(0,1)$. Find the PDF's of the following transformations: 
\begin{enumerate}
\item $U^2$ 
\item $\sqrt{U}$ 
\item $- \log(U)$
\end{enumerate}
\end{exercise}

\begin{solution}{4}
\begin{enumerate}
\item Let $Y = U^2$ and $y = u^2$. We are trying to find the PDF of $y$. We have the transformation formula: 

$$ f_Y(y) = f_U(u) \cdot \left| \frac{du}{dy}  \right|$$
We know that $u = \sqrt{y} \Rightarrow \frac{du}{dy} = \frac{1}{2 \sqrt{y}}$. Since $y$ is always positive, we can ignore the absolute value sign. Also, $f_U(u) = 1$ because $U$ is $\Unif(0,1)$. Therefore, the final answer is: 

$$ f_Y(y) = \frac{1}{2 \sqrt{y}}$$

\item Now, let $Y = \sqrt{U}$ and let $y = \sqrt{u}$, meaning $u= y^2 \Rightarrow \frac{du}{dy} = 2y$. Therefore, by the same logic as above: 

$$ f_Y(y) = 2y$$

\item Finally, let $Y = - \log U, y = - \log u$ and thus $u = e^{-y}$. Therefore, $\frac{du}{dy} = -e^{-y}$ and its absolute value would be $e^{-y}$. Therefore, we get: 

$$ f_Y(y) = e^{-y}$$ and thus $Y \sim \Expo(1)$. 
\end{enumerate}



\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Transformations of Exponentials}
Let $U, V \sim \Expo(1)$ be two independent random variables. Define $X = U + V$ and $Y = \frac{U}{U+V}$. 
\begin{enumerate}
\item Find the joint PDF of $X$ and $Y$ 

\item Find the marginal distributions of $X$ and $Y$ 
\end{enumerate}
\end{exercise}

\begin{solution}{4}
\begin{enumerate}
\item We know that $f_{U, V}(u,v) = f_{U}(u) f_{V}(v) = e^{-u}e^{-v}$ Now, in order to perform this change of variables, we need to find the determinant of the following Jacobian: 

$$ \left( \begin{array}{cc}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \\ \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}
\end{array} \right)$$

We have that $x = u + v, y = \frac{u}{u+v}$ and so we can solve for $u$ and $v$ in terms of $x$ and $y$. We get: $u = xy$ and $v = x(1-y)$ Therefore, the derivatives of the Jacobian above can be computed quite easily. We get the following: 

$$ \frac{\partial(u,v)}{\partial(x,y)} = \left| \begin{array}{cc}
 y &x  \\
 1-y&-x 
\end{array}  \right|= x$$

Therefore, the final PDF is: 

$$ f_{X, Y}(x,y) = e^{-(xy)} e^{-(x(1-y))} \cdot x = x \cdot e^{-x}$$

\item To find the marginal PDF of $X$, we integrate out $Y$. For a given value of $X$, we know that $Y$ can range from 0 to 1. To find the marginal PDF of $Y$, we integrate out $X$ noting that $X$ can be anything for a given value of $Y$. Therefore, we get the following integrals: 

$$f_X(x) =  \int_0^{1} xe^{-x} dy = xe^{-x}$$ 
$$f_Y(y) = \int_{0}^{\infty} xe^{-x} dx = 1$$ because the last integral is the expected value of the exponential!. 

Therefore, we get that $Y \sim \Unif(0,1)$. On the other hand, $X \sim \Gam(2)$, which is a distribution that we will learn about next week. We also see that $X$ and $Y$ are independent because the joint distribution factors as a product of the marginal distributions.
\end{enumerate}
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Box-Muller Transformation}
Suppose we are told that $U \sim \Unif\paren{0, 1}$ and $V \sim \Expo\paren{\frac{1}{2}}$ independently. Find the density function and thus the joint distribution of
\begin{align*}
X &= \sqrt{V} \sin\paren{2 \pi U} \\
Y &= \sqrt{V} \cos\paren{2 \pi U}
\end{align*}
\end{exercise}

\begin{solution}{3}
To do this, we can obtain the Jacobian and perform a change of variables. Note that it is easier to find the Jacobian of the transformation going for $\paren{U, V} \rightarrow \paren{X, Y}$ since $X$ and $Y$ are already expressed in terms of $U$ and $V$. The Jacobian is given as follows.
$$J = \frac{\partial(x, y)}{\partial(u, v)} = \begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \\
\end{pmatrix} = \begin{pmatrix}
\sqrt{v} \cos\paren{2 \pi u} \paren{2 \pi} & \frac{1}{2 \sqrt{v}} \sin\paren{2 \pi u} \\
- \sqrt{v} \sin\paren{2 \pi u} \paren{2 \pi} & \frac{1}{2 \sqrt{v}} \cos\paren{2 \pi u} \\
\end{pmatrix}$$
The Jacobian $|J|$ is found to be the determinant of $J$ above.
\begin{align*}
|J| &= \sqrt{v} \cos\paren{2 \pi u} \paren{2 \pi} \frac{1}{2 \sqrt{v}} \cos\paren{2 \pi u} + \sqrt{v} \sin\paren{2 \pi u} \paren{2 \pi} \frac{1}{2 \sqrt{v}} \sin\paren{2 \pi u} \\
&= \paren{\cos^2\paren{2 \pi u} + \sin^2\paren{2 \pi u}} \pi \\
&= \pi
\end{align*}
Thus, our transformation can be specified as follows. Note that $X^2 + Y^2 = V$.
\begin{align*}
f_{U, V}(u, v) &= f_{X, Y}(x, y) |J| \\
f_{X, Y}(x, y) &= \frac{1}{|J|} f_{U, V}(u, v) \\
&= \frac{1}{|J|} f_{U}(u) f_{V}(v) \\
&= \frac{1}{\pi}\frac{1}{2}\exp\paren{- \frac{1}{2}v} \\
&= \frac{1}{2\pi}\exp\paren{- \frac{x^2}{2} - \frac{y^2}{2}} \\
&= \frac{1}{\sqrt{2\pi}}\exp\paren{-\frac{x^2}{2}}\frac{1}{\sqrt{2\pi}}\exp\paren{-\frac{y^2}{2}}
\end{align*}
Hence, because we see that the pdf factors nicely into a term involving $x$ and a term involving $y$, those terms are in fact the marginal densities of $X$ and $Y$ (be sure to make sure that the normalizing constants go to the right places). We recognize the marginal densities as standard normal densities. And thus, it follows that $X$ and $Y$ are i.i.d. $\N(0, 1)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{More Beta Properties (BH 8.29)}
Let $B\sim \Beta(\alpha, \beta)$. Find the distribution of $1 - B$ in two ways by
\begin{enumerate}
\item Using a change of variables  
\item Using a story proof related to the Gamma distribution. 
\end{enumerate}
Also explain why the result makes sense in terms
of Beta being the conjugate prior for the Binomial.
\end{exercise}

\begin{solution}{4}
\vspace{-0.5mm}
\begin{enumerate}
\item Let $W = 1 - B$. Then we have $B = 1 - W$, and so $\left| \frac{db}{dw} \right| = |-1| = 1$. Hence, the PDF of $W$ is
$$f_W(w) = f_B(b) \left| \frac{db}{dw} \right| = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}b^{\alpha - 1}(1 - b)^{\beta - 1} = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}(1 - w)^{\alpha - 1}w^{\beta - 1}$$
for $0 < w < 1$. We recognize this PDF as the $\Beta(\beta, \alpha)$ distribution, and so $W \sim \Beta(\beta, \alpha)$.
\item Using the bank-post office story, we can represent $B = \frac{X}{X + Y}$ with $X \sim \Gam(\alpha, 1)$ and $Y \sim \Gam(\beta, 1)$ independent. Then $1 - B = \frac{Y}{X + Y} \sim \Beta(\beta, \alpha)$ by the same story.
\end{enumerate}
This result makes sense intuitively since if we use $\Beta(\alpha, \beta)$ as the prior distribution for the probability $p$ of success in a Binomial problem, interpreting $\alpha$ as the number of prior successes and $\beta$ as the number of prior failures, then $1 - p$ is the probability of failure and, interchanging the roles of ``success'' and ``failure,'' it makes sense to have $1 - p \sim \Beta(\beta, \alpha)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Second-best}
Let $U_1, \ldots, U_n$ be i.i.d. $\Unif(0, 1)$. Find the unconditional distribution of $U_{(n - 1)}$, and the conditional distribution of $U_{(n - 1)}$ given $U_{(n)} = c$.
\end{exercise}

\begin{solution}{3.5}
Unconditionally, $U_{(n - 1)} \sim \Beta(n - 1, 2)$, using what we know about Uniform order statistics. \\
For the conditional distribution,
\begin{align*}
P\paren{U_{(n - 1)} \leq x | U_{(n)} = c} &= P\paren{\textrm{remaining $n - 1$ Uniforms is $\leq x$} | U_{(1)} < c, \ldots, U_{(n - 1)} < c, U_{(n)} = c} \\
&= \paren{\frac{x}{c}}^{n - 1}
\end{align*}
for $0 < x < c$. Hence, the PDF is $f_{U_{(n - 1)} | U_{(n)}}(x | c) = (n - 1)\paren{\frac{x}{c}}^{n - 2} \cdot \frac{1}{c}$. This is the distribution of $cX$ where $X \sim \Beta(n - 1, 1)$, which can be easily verified with a change of variables. Hence, the conditional distribution of $U_{(n - 1)}$ is that of a scaled Beta!
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{Order Statistics}
Let $X\sim \Bin(n, p)$ and $B \sim \Beta(j, n - j +1)$, where n is a positive integer and $j$ is a positive integer with $j \leq n$. Show using a story about order statistics that $$P(X \geq j) = P(B \leq p)$$
This shows that the CDF of the continuous r.v. B is closely related to the CDF of the discrete r.v. X, and is another connection between the Beta and Binomial.
\end{exercise}

\begin{solution}{2.5}
Let $U_1, \ldots, U_n$ be i.i.d. $\Unif(0, 1)$. Think of these as Bernoulli trials, where $U_j$ is defined to be ``successful'' if $U_j < p$ (so the probability of success is $p$ for each trial). Let $X$ be the number of successes. Then $X \geq j$ is the same event as $U_{(j)} \leq p$, so $P(X \geq j) = P(U_{(j)} \leq p)$.
\end{solution}


\end{document}
